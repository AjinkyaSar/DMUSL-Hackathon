# -*- coding: utf-8 -*-
"""DMUSL_Hackathon_Group_5_Project-7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-y2pMhM447dC282aW66BwK9IxeqRl7tb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Load the dataset
df = pd.read_csv("/content/Hackathon.csv")

df.shape

df.describe()

df.info()

df.head()

df.isnull().sum()

import seaborn as sns
import matplotlib.pyplot as plt

# Visual check of missing data
plt.figure(figsize=(10, 6))
sns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')
plt.title('Missing Data Map')
plt.show()

# Check how many missing values we have first
missing_states = df['State'].isnull().sum()

if missing_states > 0:
    state_mode = df['State'].mode()[0]
    print(f"Filling {missing_states} missing states with mode: {state_mode}")
    df['State'] = df['State'].fillna(state_mode)
else:
    print("No missing values in State column.")

# 1. Handle Missing Values
# For numerical finance data, median is often safer than mean to avoid outlier bias
df['Ebitda'] = df['Ebitda'].fillna(df['Ebitda'].median())
df['Revenuegrowth'] = df['Revenuegrowth'].fillna(df['Revenuegrowth'].median())
df['Fulltimeemployees'] = df['Fulltimeemployees'].fillna(df['Fulltimeemployees'].median())

df.isnull().sum()

# Count exact duplicates
exact_duplicates = df.duplicated().sum()
print(f"Exact duplicate rows found: {exact_duplicates}")

import matplotlib.pyplot as plt
import seaborn as sns

# Identify numerical columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Define the number of rows and columns for the subplot grid
n_cols = 3
n_rows = (len(numeric_cols) + n_cols - 1) // n_cols

# Create the figure
fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 5))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    sns.boxplot(data=df, y=col, ax=axes[i], color='salmon')
    axes[i].set_title(f'Boxplot of {col}', fontsize=14)
    axes[i].set_ylabel('')

# Remove any empty subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

df.head()

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 1. Log Transformation (To make StandardScaler work with your outliers)
# This handles the extreme Marketcap/Ebitda values you saw in your boxplots
df['Log_Marketcap'] = np.log10(df['Marketcap'] + 1)
df['Log_Ebitda'] = np.log10(df['Ebitda'].abs() + 1) # .abs() because Ebitda can be negative

# 2. Ordinal Encoding for Market Cap 'Tiers'
def get_cap_tier(cap):
    if cap >= 200e9: return 4    # Mega-Cap
    elif cap >= 10e9: return 3   # Large-Cap
    elif cap >= 2e9: return 2    # Mid-Cap
    else: return 1               # Small-Cap

df['Cap_Tier'] = df['Marketcap'].apply(get_cap_tier)

# 3. Domain-Driven One-Hot Encoding for Risk Groups
risk_map = {
    'Technology': 'High_Growth', 'Communication Services': 'High_Growth',
    'Consumer Cyclical': 'Moderate', 'Financial Services': 'Moderate',
    'Industrials': 'Moderate', 'Energy': 'Moderate', 'Basic Materials': 'Moderate',
    'Healthcare': 'Defensive', 'Consumer Defensive': 'Defensive',
    'Utilities': 'Defensive', 'Real Estate': 'Defensive'
}

df['Risk_Group'] = df['Sector'].map(risk_map).fillna('Other')
df = pd.get_dummies(df, columns=['Risk_Group'], prefix='Group')

# 4. Financial Health Score (Boolean)
df['Quality_Score'] = ((df['Ebitda'] > 0) & (df['Revenuegrowth'] > 0)).astype(int)

# Select the final features for clustering
# Note: We use Log_Marketcap instead of raw Marketcap
final_features = [
    'Weight',
    'Revenuegrowth',
    'Log_Marketcap',
    'Log_Ebitda',
    'Cap_Tier',
    'Quality_Score'
] + [col for col in df.columns if 'Group_' in col]

X = df[final_features]

# Applying StandardScaler (as taught in class)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convert back to DataFrame for readability
X_scaled_df = pd.DataFrame(X_scaled, columns=final_features)
X_scaled_df.head()

import matplotlib.pyplot as plt
import seaborn as sns

cols_to_plot = ['Currentprice', 'Marketcap', 'Revenuegrowth', 'Weight']

plt.figure(figsize=(15, 10))
for i, col in enumerate(cols_to_plot):
    plt.subplot(2, 2, i+1)
    sns.histplot(df[col], kde=True, color='teal')
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Correlation Heatmap
plt.figure(figsize=(10, 8))
numeric_df = df.select_dtypes(include=[np.number])
sns.heatmap(numeric_df.corr(), annot=True, cmap='RdYlGn', fmt='.2f')
plt.title('Financial Feature Correlation Matrix')
plt.show()

# Scatter Plot: Market Cap vs Revenue Growth
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x='Marketcap', y='Revenuegrowth', hue='Sector', alpha=0.7)
plt.xscale('log') # Essential for visualizing outliers alongside small caps
plt.title('Bivariate Analysis: Market Cap vs. Revenue Growth')
plt.grid(True, which="both", ls="-", alpha=0.5)
plt.show()

# Boxplot of Revenue Growth by Sector
plt.figure(figsize=(12, 6))
sns.boxplot(data=df, x='Sector', y='Revenuegrowth', palette='Set3')
plt.xticks(rotation=45)
plt.title('Multivariate Analysis: Revenue Growth Variance by Sector')
plt.show()

# Pairplot for high-level interaction
# We'll pick 3-4 key variables to avoid a messy plot
sns.pairplot(df[['Log_Marketcap', 'Revenuegrowth', 'Weight', 'Sector']], hue='Sector', palette='bright')
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# We use the X_scaled from our previous step
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plotting the Elbow Graph
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method to Find Optimal Clusters')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

# 1. Fit the KMeans model with k=5
optimal_k = 5
kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42)
df['Cluster'] = kmeans.fit_predict(X_scaled)

# 2. Analyze the clusters based on our original financial features
# We look at the MEAN of each feature per cluster to define the "Profile"
cluster_summary = df.groupby('Cluster').agg({
    'Revenuegrowth': 'mean',
    'Marketcap': 'mean',
    'Ebitda': 'mean',
    'Weight': 'mean',
    'Cap_Tier': 'mean'
}).sort_values(by='Marketcap', ascending=False) # Sorting by size often helps identify safety

print("Cluster Profiles for Advisory System:")
display(cluster_summary)

from sklearn.ensemble import RandomForestClassifier

# We use the Cluster labels we just generated as the 'target'
# to see which features best define those groups
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_scaled, df['Cluster'])

# Get Feature Importances
importances = rf.feature_importances_
feature_names = final_features

# Plotting
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=feature_names, palette='magma')
plt.title('Feature Importance: Why we chose these variables')
plt.xlabel('Importance Score')
plt.ylabel('Features')
plt.show()

# Select alternative features to see if new patterns emerge
alt_features = ['Log_Ebitda', 'Weight', 'Revenuegrowth', 'Cluster']

sns.pairplot(df[alt_features], hue='Cluster', palette='coolwarm', diag_kind='kde')
plt.suptitle('Multivariate Analysis: Finding New Cluster Patterns', y=1.02)
plt.show()

plt.figure(figsize=(12, 7))
sns.scatterplot(
    data=df,
    x='Log_Marketcap',
    y='Revenuegrowth',
    hue='Cluster',
    palette='Set1',
    size='Weight',
    sizes=(20, 400),
    alpha=0.6
)

plt.title('Final Portfolio Segmentation: Size vs. Growth', fontsize=15)
plt.xlabel('Market Cap (Log Scale)', fontsize=12)
plt.ylabel('Revenue Growth (Return Proxy)', fontsize=12)
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Risk Profiles')
plt.grid(True, linestyle='--', alpha=0.5)
plt.show()

def get_recommendations(risk_appetite):
    """
    risk_appetite: 'Low', 'Medium', 'High'
    """
    if risk_appetite == 'Low':
        # Recommend Cluster 2 (Stable Mega-Caps)
        target_cluster = 2
        advice = "Focus on capital preservation with high-quality, large-cap stabilizers."
    elif risk_appetite == 'High':
        # Recommend Cluster 1 (High Growth)
        target_cluster = 1
        advice = "Focus on aggressive growth through high-revenue-expansion assets."
    else:
        # Recommend mid-tier clusters
        target_cluster = 0 # or 3, 4
        advice = "Maintain a balanced approach with moderate growth and diversified risk."

    recs = df[df['Cluster'] == target_cluster][['Symbol', 'Shortname', 'Weight']].head(10)
    return advice, recs

# Example usage:
advice, stocks = get_recommendations('High')
print(f"Advice: {advice}")
display(stocks)

import pickle
from google.colab import files

# 1. Save the K-Means Model
with open('portfolio_cluster_model.pkl', 'wb') as f:
    pickle.dump(kmeans, f)

# 2. Save the Scaler (Crucial for consistent deployment)
with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# 3. Save the Feature Names (To ensure Pycharm uses the same column order)
with open('feature_names.pkl', 'wb') as f:
    pickle.dump(final_features, f)

# Download files to your local machine for use in PyCharm
files.download('portfolio_cluster_model.pkl')
files.download('scaler.pkl')
files.download('feature_names.pkl')

